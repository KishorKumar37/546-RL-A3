{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I [Total: 60 points] - Implementing Advantage Actor Critic (A2C) and Solving Simple Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import gymnasium as gym\n",
    "from itertools import count\n",
    "from torch import multiprocessing as mp\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.set_start_method('fork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_states,\n",
    "                 n_actions):\n",
    "        super().__init__()\n",
    "        self.shared_fc = nn.Sequential(\n",
    "            nn.Linear(n_states, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(64, n_actions),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        self.critic = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.shared_fc(x)\n",
    "        return self.actor(x), self.critic(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A3CAgent():\n",
    "    def __init__(self,\n",
    "                 env_name,\n",
    "                 global_model,\n",
    "                 num_workers,\n",
    "                 device,\n",
    "                 lr,\n",
    "                 max_episodes,\n",
    "                 discount_factor):\n",
    "        self.env_name = env_name\n",
    "        self.global_model = global_model\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.discount_factor = discount_factor\n",
    "        self.max_episodes = max_episodes\n",
    "        self.device = device\n",
    "\n",
    "        self.episode_rewards = []\n",
    "        self.episode_steps = []\n",
    "        self.episode_losses = []\n",
    "\n",
    "        print(\"Finished initializing of A3C agent\")\n",
    "    \n",
    "    def train(self):\n",
    "        envs = [gym.make(self.env_name) for _ in range(self.num_workers)]\n",
    "        worker_models = [ActorCritic(n_states=envs[0].observation_space.shape[0],\n",
    "                                     n_actions=envs[0].action_space.n)\n",
    "                                     for _ in range(self.num_workers)]\n",
    "        \n",
    "        for i, (env, worker_model) in enumerate(zip(envs, worker_models)):\n",
    "            #  print(\"Entered multiprocess loop\")\n",
    "             worker = mp.Process(target=self.train_worker,\n",
    "                                 args=(env,\n",
    "                                       self.global_model,\n",
    "                                       worker_model,\n",
    "                                       self.lr,\n",
    "                                       self.max_episodes,\n",
    "                                       i))\n",
    "             worker.start()\n",
    "            #  print(\"Executed worker\")\n",
    "\n",
    "    def train_worker(self,\n",
    "                     env,\n",
    "                     global_model,\n",
    "                     worker_model,\n",
    "                     lr,\n",
    "                     max_episodes,\n",
    "                     worker_name):\n",
    "        print(f\"Worker {worker_name} initiated\")\n",
    "        ''' Kishorkumar Devasenapathy - 04-15-2024\n",
    "        # This optimizer is only in the scope of this worker but has a shared copy of the global model's\n",
    "        # parameters to perform gradient descent and update\n",
    "        # torch.multiprocessing takes care of synchronizing this step across the workers\n",
    "        '''\n",
    "        optimizer = torch.optim.Adam(global_model.parameters(),\n",
    "                                     lr=lr)\n",
    "        print(f\"Worker {worker_name} initialized optimizer for global model\")\n",
    "        actor_loss_fn = nn.LogSoftmax()\n",
    "        critic_loss_fn = nn.MSELoss()\n",
    "\n",
    "        state, _ = env.reset()\n",
    "        state = torch.tensor(state,\n",
    "                             dtype=torch.float32,\n",
    "                             device=self.device).unsqueeze(0)\n",
    "        print(f\"Worker {worker_name} interacted first time with environment\")\n",
    "        episode_reward = 0\n",
    "        episode_loss = 0\n",
    "\n",
    "        for _ in range(max_episodes):\n",
    "            print(f\"Worker {worker_name} started an episode\")\n",
    "            worker_model.load_state_dict(global_model.state_dict()) # synchronize worker's model with that of global model\n",
    "\n",
    "            state, _ = env.reset()\n",
    "            state = torch.tensor(state,\n",
    "                                 dtype=torch.float32,\n",
    "                                 device=self.device)\n",
    "            print(state.shape)\n",
    "\n",
    "            for step in count():\n",
    "                action_probs, current_state_value = worker_model.forward(state)\n",
    "                print(action_probs.shape)\n",
    "                action = torch.max(action_probs,\n",
    "                                   dim=-1).indices\n",
    "                print(action.shape)\n",
    "\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "                next_state = torch.tensor(state,\n",
    "                                          dtype=torch.float32,\n",
    "                                          device=self.device)\n",
    "                _, next_state_value = worker_model.forward(next_state)\n",
    "\n",
    "                episode_reward += reward\n",
    "\n",
    "                # reward = torch.tensor([reward],\n",
    "                #                       dtype=torch.float32,\n",
    "                #                       device=self.device)\n",
    "                # print(reward.shape)\n",
    "\n",
    "                # reward *= 5\n",
    "\n",
    "                if terminated:\n",
    "                    target_current_state_value = reward\n",
    "                else:\n",
    "                    target_current_state_value = reward + self.discount_factor*next_state_value.item()\n",
    "\n",
    "                # actor_loss = -torch.log_softmax(action_probs,\n",
    "                #                                 dim=-1)\n",
    "                # actor_loss = actor_loss_fn(action_probs)\n",
    "                # critic_loss = critic_loss_fn(target_current_state_value, current_state_value.item())\n",
    "                advantage = target_current_state_value - current_state_value\n",
    "\n",
    "                actor_loss = -torch.log_softmax(action_probs, dim=-1)[action] * advantage\n",
    "                critic_loss = 0.5 * advantage**2\n",
    "\n",
    "                episode_loss += (actor_loss + critic_loss)\n",
    "                \n",
    "                state = next_state\n",
    "\n",
    "                if terminated or truncated:\n",
    "                    print(f\"Worker {worker_name} completed an episode\")\n",
    "                    self.episode_rewards.append(episode_reward)\n",
    "                    self.episode_steps.append(step+1)\n",
    "                    self.episode_losses.append(episode_loss)\n",
    "                    break\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            episode_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f\"Worker {worker_name} completed\")\n",
    "\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME        = \"CartPole-v1\"\n",
    "NUM_WORKERS     = 2\n",
    "\n",
    "LR              = 1e-4\n",
    "MAX_EPISODES    = 5\n",
    "\n",
    "DISCOUNT_FACTOR = 0.6\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "# elif torch.backends.mps.is_available():\n",
    "#     DEVICE = \"mps\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished initializing of A3C agent\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(ENV_NAME)\n",
    "global_model = ActorCritic(n_states=env.observation_space.shape[0],\n",
    "                           n_actions=env.action_space.n)\n",
    "global_model.share_memory()\n",
    "\n",
    "trainer = A3CAgent(env_name=ENV_NAME,\n",
    "                   global_model=global_model,\n",
    "                   num_workers=NUM_WORKERS,\n",
    "                   device=DEVICE,\n",
    "                   lr=LR,\n",
    "                   max_episodes=MAX_EPISODES,\n",
    "                   discount_factor=DISCOUNT_FACTOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 0 initiated"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 1 initiated\n",
      "\n",
      "Worker 1 initialized optimizer for global modelWorker 0 initialized optimizer for global model\n",
      "\n",
      "Worker 0 interacted first time with environmentWorker 1 interacted first time with environment\n",
      "\n",
      "Worker 1 started an episodeWorker 0 started an episode\n",
      "\n",
      "torch.Size([4])torch.Size([4])\n",
      "\n",
      "torch.Size([2])torch.Size([2])\n",
      "\n",
      "torch.Size([])torch.Size([])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nw/mc3sp5812dlb_5m_j3v1zqx00000gn/T/ipykernel_39680/939267076.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  next_state = torch.tensor(state,\n",
      "/var/folders/nw/mc3sp5812dlb_5m_j3v1zqx00000gn/T/ipykernel_39680/939267076.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  next_state = torch.tensor(state,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])torch.Size([2])\n",
      "\n",
      "torch.Size([])torch.Size([])\n",
      "\n",
      "torch.Size([2])torch.Size([2])\n",
      "\n",
      "torch.Size([])torch.Size([])\n",
      "\n",
      "torch.Size([2])torch.Size([2])\n",
      "\n",
      "torch.Size([])torch.Size([])\n",
      "\n",
      "torch.Size([2])torch.Size([2])\n",
      "\n",
      "torch.Size([])torch.Size([])\n",
      "\n",
      "torch.Size([2])torch.Size([2])\n",
      "\n",
      "torch.Size([])torch.Size([])\n",
      "\n",
      "torch.Size([2])torch.Size([2])\n",
      "\n",
      "torch.Size([])torch.Size([])\n",
      "\n",
      "torch.Size([2])torch.Size([2])\n",
      "\n",
      "torch.Size([])torch.Size([])\n",
      "\n",
      "Worker 0 completed an episodetorch.Size([2])\n",
      "\n",
      "torch.Size([])\n",
      "torch.Size([2])\n",
      "torch.Size([])\n",
      "Worker 1 completed an episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[39686]: +[MPSGraphObject initialize] may have been in progress in another thread when fork() was called.\n",
      "objc[39686]: +[MPSGraphObject initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.\n",
      "objc[39687]: +[MPSGraphObject initialize] may have been in progress in another thread when fork() was called.\n",
      "objc[39687]: +[MPSGraphObject initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
