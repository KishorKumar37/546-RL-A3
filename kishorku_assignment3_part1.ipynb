{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I [Total: 60 points] - Implementing Advantage Actor Critic (A2C) and Solving Simple Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import gymnasium as gym\n",
    "from itertools import count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_states,\n",
    "                 n_actions):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(n_states, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions),\n",
    "            nn.Softmax(dim=-1),\n",
    "            nn.max(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_states,\n",
    "                 n_actions):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(n_states, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "state, _ = env.reset()\n",
    "del env\n",
    "\n",
    "global_actor = Actor(n_states=len(state),\n",
    "                     n_actions=env.action_space.n)\n",
    "global_critic = Critic(n_states=len(state),\n",
    "                       n_actions=env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (1317544345.py, line 60)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 60\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class Agent():\n",
    "    def __int__(self,\n",
    "                env,\n",
    "                device,\n",
    "                lr,\n",
    "                discount_factor):\n",
    "        self.env = gym.make(env)\n",
    "        state, _ = self.env.reset()\n",
    "        self.device = device\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        self.actor = Actor(n_states=len(state),\n",
    "                           n_actions=env.action_space.n).to(device)\n",
    "        self.critic = Critic(n_states=len(state),\n",
    "                             n_actions=env.action_space.n).to(device)\n",
    "\n",
    "        self.actor_loss_fn = nn.SmoothL1Loss()\n",
    "        self.critic_loss_fn = nn.MSELoss()\n",
    "\n",
    "        self.actor_optimizer = torch.optim.AdamW(self.actor.parameters(),\n",
    "                                                 lr=lr,\n",
    "                                                 amsgrad=True)\n",
    "        self.critic_optimizer = torch.optim.AdamW(self.critic.parameters(),\n",
    "                                                  lr=lr,\n",
    "                                                  amsgrad=True)\n",
    "\n",
    "        global global_actor\n",
    "        global global_critic\n",
    "\n",
    "    def reset_gradients(self):\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        self.critic_optimizer.zero_grad()\n",
    "\n",
    "    def synchronize_threads(self):\n",
    "        self.actor.load_state_dict(global_actor.state_dict())\n",
    "        self.critic.load_state_dict(global_critic.state_dict())\n",
    "\n",
    "    def run_episode(self, state):\n",
    "        episode_rewards = []\n",
    "        # episode_observations = []\n",
    "        episode_states = []\n",
    "\n",
    "        for step in count():\n",
    "                action = self.actor.forward(state)\n",
    "                observation, reward, terminated, truncated, _ = self.env.step(action.item())\n",
    "\n",
    "                reward = torch.tensor([[reward]],\n",
    "                                      dtype=torch.float32,\n",
    "                                      device=self.device)\n",
    "                # if terminated:\n",
    "                #     observation = None\n",
    "                # else:\n",
    "                #     observation = torch.tensor(observation,\n",
    "                #                                dtype=torch.float32,\n",
    "                #                                device=self.device)\n",
    "\n",
    "                episode_rewards.append(reward)\n",
    "                # episode_observations.append(observation)\n",
    "                episode_states.append(state)\n",
    "\n",
    "                if terminated:\n",
    "                    R = torch.tensor(0)\n",
    "                    break\n",
    "                elif truncated:\n",
    "                    R = self.critic.forward(observation)\n",
    "                    break\n",
    "                else:\n",
    "                    state = torch.tensor(observation,\n",
    "                                         dtype=torch.float32,\n",
    "                                         device=self.device)\n",
    "        \n",
    "        return episode_states, episode_rewards, step, R\n",
    "\n",
    "    def accumulate_gradients(self, episode_states, episode_rewards, episode_steps, R):\n",
    "        for t in range(episode_steps, -1, -1):\n",
    "                R *= self.discount_factor\n",
    "                R += episode_rewards[t]\n",
    "\n",
    "                state_value = self.critic.forward(episode_states[t])\n",
    "\n",
    "                actor_loss = 0\n",
    "                actor_loss.backward()\n",
    "                # self.actor_optimizer.zero_grad()\n",
    "                # self.actor_optimizer.step()\n",
    "\n",
    "                critic_loss = self.critic_loss_fn(state_value, R)\n",
    "                critic_loss.backward()\n",
    "                # self.critic_optimizer.zero_grad()\n",
    "                # self.critic_optimizer.step()\n",
    "    \n",
    "\n",
    "    def train(self, episodes):\n",
    "        for episode in range(episodes):\n",
    "            self.reset_gradients()\n",
    "            self.synchronize_threads()\n",
    "\n",
    "            state, _ = self.env.reset()\n",
    "            state = torch.tensor(state,\n",
    "                                 dtype=torch.float32,\n",
    "                                 device=self.device)\n",
    "            \n",
    "            episode_states, episode_rewards, episode_steps, R = self.run_episode(state)\n",
    "            self.accumulate_gradients(episode_states, episode_rewards, episode_steps, R)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor(0)\n",
    "a.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
